{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Product Name Brand Name   Price  \\\n",
      "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
      "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
      "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
      "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
      "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
      "\n",
      "   Rating                                            Reviews  Review Votes  \n",
      "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
      "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
      "2       5                                       Very pleased           0.0  \n",
      "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
      "4       4  Great phone to replace my lost phone. The only...           0.0  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Amazon_reviews.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308277, 7)\n",
      "0.7482686025879323\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace = True)\n",
    "data = data[data['Rating'] != 3]\n",
    "data['Sentiment'] = np.where(data['Rating'] > 3, 1, 0)\n",
    "print(data.shape)\n",
    "print(data['Sentiment'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (200456,)\n",
      "X_val: (61579,)\n",
      "X_test: (46242,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Reviews'], data['Sentiment'], test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.235, random_state=1)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', 'anounced', 'chartered', 'displaythe', 'functionsmeasurementslength', 'itits', 'nand', 'primera', 'selectboard', 'thid', 'wouuuuuu']\n",
      "Feature count: 50863\n"
     ]
    }
   ],
   "source": [
    "countVect = CountVectorizer().fit(X_train)\n",
    "print(countVect.get_feature_names()[::5000])\n",
    "print(\"Feature count:\", len(countVect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = countVect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=200, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C = 200)\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(countVect.transform(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9521590152487049\n",
      "f1_score: 0.9517986245433183\n",
      "average_precision_score(weighted): 0.9560651601937877\n",
      "recall_score(weighted): 0.9521590152487049\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_val, predictions))\n",
    "print(\"f1_score:\", f1_score(y_val, predictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_val, predictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_val, predictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredictions = model.predict(countVect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9523809523809523\n",
      "f1_score(weighted): 0.9520365125338477\n",
      "average_precision_score(weighted): 0.9561505149836593\n",
      "recall_score(weighted): 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_test, testPredictions))\n",
    "print(\"f1_score(weighted):\", f1_score(y_test, testPredictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_test, testPredictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_test, testPredictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['mony' 'false' 'worst' 'horribly' 'unsatisfied' 'worthless' 'nope'\n",
      " 'messing' 'nit' 'lemon']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' 'excelente' '4eeeks' 'superb' 'efficient' 'exelente'\n",
      " 'pleasantly' 'lovely' 'matching' 'satisfy']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(countVect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,#;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z _]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, \" \", text)\n",
    "    text = re.sub(BAD_SYMBOLS_RE, \"\", text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_sentence = \"\"\n",
    "    for w in text_tokens:\n",
    "        if w not in STOPWORDS:\n",
    "            filtered_sentence += w + \" \"\n",
    "    return filtered_sentence[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = [text_prepare(x) for x in X_train]\n",
    "X2_val = [text_prepare(x) for x in X_val]\n",
    "X2_test = [text_prepare(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('phone', 209054), ('great', 51217), ('good', 47339), ('one', 31106), ('like', 28646), ('screen', 27201), ('use', 26345), ('battery', 25525), ('works', 24194), ('love', 23325)]\n"
     ]
    }
   ],
   "source": [
    "words_counts = {}\n",
    "\n",
    "for review in X2_train:\n",
    "    for word in review.split():\n",
    "        words_counts[word] = words_counts.setdefault(word, 0) + 1\n",
    "\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:DICT_SIZE]\n",
    "\n",
    "WORDS_TO_INDEX = {}\n",
    "i = 0\n",
    "for entry in most_common_words:\n",
    "    WORDS_TO_INDEX[entry[0]] = i\n",
    "    i += 1\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for word in text.split():\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] += 1\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (200456, 5000)\n",
      "X_val shape  (61579, 5000)\n",
      "X_test shape  (46242, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X2_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X2_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X2_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C = 200).fit(X_train_mybag, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = classifier.predict(X_val_mybag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9364556098670002\n",
      "f1_score(weighted): 0.935654715781444\n",
      "average_precision_score(weighted): 0.9408343638316503\n",
      "recall_score(weighted): 0.9364556098670002\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_val, predictions2))\n",
    "print(\"f1_score(weighted):\", f1_score(y_val, predictions2, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_val, predictions2, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_val, predictions2, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredictions2 = classifier.predict(X_test_mybag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9354699191211453\n",
      "f1_score(weighted): 0.9345606685708138\n",
      "average_precision_score(weighted): 0.9388241660868866\n",
      "recall_score(weighted): 0.9354699191211453\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_test, testPredictions2))\n",
    "print(\"f1_score(weighted):\", f1_score(y_test, testPredictions2, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_test, testPredictions2, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_test, testPredictions2, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTrialset = ['Bad phone']\\nX2_trial = [text_prepare(x) for x in Trialset]\\nX_trial_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X2_trial])\\nprint(classifier.predict(countVect.transform(['phone is good', 'not good'])))\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Trialset = ['Bad phone']\n",
    "X2_trial = [text_prepare(x) for x in Trialset]\n",
    "X_trial_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X2_trial])\n",
    "print(classifier.predict(countVect.transform(['phone is good', 'not good'])))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16725\n"
     ]
    }
   ],
   "source": [
    "tfidfVect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "print(len(tfidfVect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfX_train_vectorized = tfidfVect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmodel = LogisticRegression(C = 50)\n",
    "tfmodel.fit(tfX_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfpredictions = tfmodel.predict(tfidfVect.transform(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.955861576186687\n",
      "f1_score(weighted): 0.9557326389086596\n",
      "average_precision_score(weighted): 0.9619931937214505\n",
      "recall_score(weighted): 0.955861576186687\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_val, tfpredictions))\n",
    "print(\"f1_score(weighted):\", f1_score(y_val, tfpredictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_val, tfpredictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_val, tfpredictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTfpredictions = tfmodel.predict(tfidfVect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9554085030924268\n",
      "f1_score(weighted): 0.9552966629601434\n",
      "average_precision_score(weighted): 0.9616930026787053\n",
      "recall_score(weighted): 0.9554085030924268\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_test, testTfpredictions))\n",
    "print(\"f1_score(weighted):\", f1_score(y_test, testTfpredictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_test, testTfpredictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_test, testTfpredictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['1300' 'messiah' 'v7' 'keynote' 'ionized' 'bigtime' 'hosts' 'brawns'\n",
      " 'bridging' '1b']\n",
      "\n",
      "Largest tfidf: \n",
      "['buen' 'top' 'fire' 'fits' 'brilliant' 'five' 'flimsy' 'too' 'a1' 'bravo']\n"
     ]
    }
   ],
   "source": [
    "tffeature_names = np.array(tfidfVect.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = tfX_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(tffeature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(tffeature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['worst' 'mony' 'jun' 'prediction' 'theory' 'false' 'remembering' 'pos'\n",
      " 'nope' 'expiration']\n",
      "\n",
      "Largest Coefs: \n",
      "['wacky' 'love' 'aviv' '4eeeks' 'great' 'tact' 'pleasantly' 'hesitate'\n",
      " 'excellent' 'amazing']\n"
     ]
    }
   ],
   "source": [
    "tfsorted_coef_index = tfmodel.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(tffeature_names[tfsorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(tffeature_names[tfsorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "print(tfmodel.predict(tfidfVect.transform(['love',\n",
    "                                    'working'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179049\n"
     ]
    }
   ],
   "source": [
    "ngramVect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "print(len(ngramVect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramX_train_vectorized = ngramVect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=15, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramModel = LogisticRegression(C = 15)\n",
    "ngramModel.fit(ngramX_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramPredictions = ngramModel.predict(ngramVect.transform(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.975527371344127\n",
      "f1_score(weighted): 0.9754771704157668\n",
      "average_precision_score(weighted): 0.9783579085479265\n",
      "recall_score(weighted): 0.975527371344127\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_val, ngramPredictions))\n",
    "print(\"f1_score(weighted):\", f1_score(y_val, ngramPredictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_val, ngramPredictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_val, ngramPredictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNgramPredictions = ngramModel.predict(ngramVect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9761688508282513\n",
      "f1_score(weighted): 0.9761118295595008\n",
      "average_precision_score(weighted): 0.9785059251371223\n",
      "recall_score(weighted): 0.9761688508282513\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_test, testNgramPredictions))\n",
    "print(\"f1_score(weighted):\", f1_score(y_test, testNgramPredictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_test, testNgramPredictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_test, testNgramPredictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['good get' 'perfect no' 'knot' 'junk' 'middling' 'to happy' 'one star'\n",
      " 'holster' 'garbage' 'worst']\n",
      "\n",
      "Largest Coefs: \n",
      "['best not' 'not bad' 'no problems' 'all like' 'pictures quality' 'no bad'\n",
      " 'no issues' 'not too' 'no problem' 'bad it']\n"
     ]
    }
   ],
   "source": [
    "ngramfeature_names = np.array(ngramVect.get_feature_names())\n",
    "\n",
    "ngramsorted_coef_index = ngramModel.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(ngramfeature_names[ngramsorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(ngramfeature_names[ngramsorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(ngramModel.predict(ngramVect.transform(['phone is good',\n",
    "                                    'an issue, phone is not working', 'horrible phone'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfVect = CountVectorizer(min_df=5).fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfX_train_vectorized = clfVect.transform(X_train) \n",
    "print(\"Feature count:\", len(countVect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.LinearSVC()\n",
    "clf.fit(clfX_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2= svm.NuSVC(kernel = 'poly, degree = 2)\n",
    "clf2.fit(clfX_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfpredictions = clf.predict(clfVect.transform(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9521102973416262\n",
      "f1_score(weighted): 0.9518790080001326\n",
      "average_precision_score(weighted): 0.9577655231535342\n",
      "recall_score(weighted): 0.9521102973416262\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_val, clfpredictions))\n",
    "print(\"f1_score(weighted):\", f1_score(y_val, clfpredictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_val, clfpredictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_val, clfpredictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestClfpredictions = clf.predict(clfVect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9533108429566195\n",
      "f1_score(weighted): 0.9530618905281582\n",
      "average_precision_score(weighted): 0.9581256805105789\n",
      "recall_score(weighted): 0.9533108429566195\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", accuracy_score(y_test, TestClfpredictions))\n",
    "print(\"f1_score(weighted):\", f1_score(y_test, TestClfpredictions, average = 'weighted'))\n",
    "print(\"average_precision_score(weighted):\", average_precision_score(y_test, TestClfpredictions, average = 'weighted'))\n",
    "print(\"recall_score(weighted):\", recall_score(y_test, TestClfpredictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clffeature_names = np.array(clfVect.get_feature_names())\n",
    "\n",
    "clfsorted_coef_index = clf.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(clffeature_names[clfsorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(clffeature_names[clfsorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repeat = True\n",
    "while repeat == True:\n",
    "    string = input(\"Enter custom string:(0 to exit)\")\n",
    "    if string != '0':\n",
    "        print(ngramModel.predict(ngramVect.transform([string])))\n",
    "    else:\n",
    "        repeat = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
